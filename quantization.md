
# ðŸ§© Understanding Quantization Options in Ollama

Quantization compresses a modelâ€™s weights to use fewer bits, reducing memory requirements and making large models run on smaller GPUs. Ollama supports several quantization schemes, each balancing **VRAM usage, speed, and accuracy**.

---

## ðŸ”Ž Available Quantization Options

| Option     | Precision | VRAM (12B model) | Accuracy | Notes |
|------------|-----------|------------------|----------|-------|
| **Q2_K**   | 2â€‘bit     | ~6â€“8â€¯GB          | Lowest   | Extreme compression, fastest but least accurate |
| **Q3_K_M** | 3â€‘bit     | ~8â€“10â€¯GB         | Low      | Efficient, moderate accuracy |
| **Q3_K_L** | 3â€‘bit     | ~9â€“11â€¯GB         | Low+     | Slightly better than Q3_K_M |
| **Q4_0**   | 4â€‘bit     | ~10â€“12â€¯GB        | Medium   | Basic 4â€‘bit quantization |
| **Q4_1**   | 4â€‘bit     | ~11â€“13â€¯GB        | Medium+  | Higher accuracy than Q4_0 |
| **Q4_K_M** | 4â€‘bit     | ~8â€“10â€¯GB         | Medium   | Most common default, balanced speed/accuracy |
| **Q4_K_S** | 4â€‘bit     | ~9â€“11â€¯GB         | Medium+  | Slightly slower, higher accuracy |
| **Q5_0**   | 5â€‘bit     | ~12â€“14â€¯GB        | High     | Good balance of accuracy and size |
| **Q5_1**   | 5â€‘bit     | ~13â€“15â€¯GB        | High+    | Better accuracy than Q5_0 |
| **Q5_K_M** | 5â€‘bit     | ~12â€“14â€¯GB        | High     | Popular choice for midâ€‘range GPUs |
| **Q5_K_S** | 5â€‘bit     | ~13â€“15â€¯GB        | High+    | Slightly better accuracy |
| **Q6_K**   | 6â€‘bit     | ~16â€“18â€¯GB        | Very High| Larger footprint, near FP16 accuracy |
| **Q8_0**   | 8â€‘bit     | ~24â€¯GB           | Highest  | Near fullâ€‘precision accuracy, requires highâ€‘end GPU |

---

## âš¡ Default Quantization Value
- For **large models** (like `gemma3:12b` or `gemma4:12b`), Ollama **defaults to `Q4_K_M`**.  
- This choice balances **VRAM efficiency (~8â€“10â€¯GB)** with **reasonable accuracy**, making it accessible to most consumer GPUs.  
- If you want higher accuracy and have more VRAM, you can override the default by specifying `options.quantize` in your API call or Modelfile.

---

## ðŸŒ± Practical Example: Gemmaâ€¯3:12B on 16â€¯GB VRAM / 40â€¯GB RAM
Imagine you have a system with **16â€¯GB GPU VRAM** and **40â€¯GB system RAM**.  

- **Best quantization**: `Q5_K_M`  
  - Requires ~12â€“14â€¯GB VRAM â†’ fits comfortably in 16â€¯GB.  
  - Uses ~30â€“40â€¯GB RAM â†’ matches your system capacity.  
  - Provides stronger accuracy than the default `Q4_K_M`.  
- **Alternatives**:  
  - `Q4_K_M` â†’ safer, lighter, but slightly less accurate.  
  - `Q8_0` â†’ too heavy (needs ~24â€¯GB VRAM).  
  - `Q6_K` â†’ borderline fit, may cause instability.  

### Node.js Example
```js
const response = await fetch(`http://${process.env.HOST}:${process.env.PORT}/api/chat`, {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    model: "gemma3:12b",
    messages,
    options: {
      quantize: "Q5_K_M"   // best match for 16GB VRAM / 40GB RAM
    }
  })
});

const data = await response.json();
console.log(data);
```

âœ… **Conclusion**: On a 16â€¯GB GPU with 40â€¯GB RAM, `Gemmaâ€¯3:12B` with **`Q5_K_M` quantization** is the sweet spot â€” balancing accuracy and efficiency while fitting your hardware perfectly.

